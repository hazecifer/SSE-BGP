{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('Encoded_train_data.csv')\n",
    "X_test = pd.read_csv('Encoded_test_data.csv')\n",
    "y_train = pd.read_csv('y_train.csv').squeeze()\n",
    "y_test = pd.read_csv('y_test.csv').squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim//2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class PyTorchRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, hidden_dim=256, dropout=0.2, lr=1e-3, epochs=100, batch_size=32):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.FloatTensor(y.values if isinstance(y, pd.Series) else y).view(-1, 1)\n",
    "        \n",
    "        self.model = TorchModel(X.shape[1], self.hidden_dim, self.dropout)\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            for inputs, targets in loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        with torch.no_grad():\n",
    "            return self.model(torch.FloatTensor(X)).numpy().flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Define the paramspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_spaces = {\n",
    "    'rf': {\n",
    "        'n_estimators': Integer(100, 300),\n",
    "        'max_depth': Integer(3, 20),\n",
    "        'min_samples_split': Integer(2, 10),\n",
    "        'min_samples_leaf':Integer(1,10)\n",
    "    },\n",
    "    'xgb': {\n",
    "        'n_estimators': Integer(100, 300),\n",
    "        'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "        'max_depth': Integer(3, 10),\n",
    "        'subsample': Real(0.5, 1.0),\n",
    "        'colsample_bytree':Real(0.5, 1.0)\n",
    "    },\n",
    "    'catboost': {\n",
    "        'iterations': Integer(100, 300),\n",
    "        'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "        'depth': Integer(3, 10),\n",
    "    },\n",
    "    'gbr': {\n",
    "        'n_estimators': Integer(100, 300),\n",
    "        'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "        'max_depth': Integer(3, 10),\n",
    "        'subsample': Real(0.5, 1.0)\n",
    "    },\n",
    "    'lgbm': {\n",
    "        'n_estimators': Integer(100, 300),\n",
    "        'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "        'num_leaves': Integer(20, 100),\n",
    "        'max_depth': Integer(3, 10)\n",
    "    },\n",
    "    'torch': {\n",
    "        'hidden_dim': Integer(128, 512),\n",
    "        'dropout': Real(0.1, 0.5),\n",
    "        'lr': Real(1e-4, 1e-2, prior='log-uniform'),\n",
    "        'epochs': Integer(50, 250),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'rf': RandomForestRegressor(random_state=42),\n",
    "    'xgb': XGBRegressor(random_state=42),\n",
    "    'catboost': CatBoostRegressor(silent=True, random_state=42),\n",
    "    'gbr': GradientBoostingRegressor(random_state=42),\n",
    "    'lgbm': LGBMRegressor(random_state=42),\n",
    "    'torch': PyTorchRegressor()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_models = {}\n",
    "for name in models:\n",
    "    print(f\"\\n{'='*40}\\nOptimizing {name}\\n{'='*40}\")\n",
    "    opt = BayesSearchCV(\n",
    "        estimator=models[name],\n",
    "        search_spaces=param_spaces[name],\n",
    "        n_iter=30,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        opt.fit(X_train, y_train)\n",
    "        optimized_models[name] = opt.best_estimator_\n",
    "        print(f\"Best params for {name}:\")\n",
    "        print(opt.best_params_)\n",
    "        print(f\"Best score: {opt.best_score_:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error optimizing {name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFinal Model Performance:\")\n",
    "for name, model in optimized_models.items():\n",
    "    preds = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    print(f\"{name.upper():<10} RMSE: {rmse:.4f} | R²: {r2:.4f}\")\n",
    "\n",
    "\n",
    "stack = StackingRegressor(\n",
    "    estimators=list(optimized_models.items())[:-1],\n",
    "    final_estimator=optimized_models['torch']\n",
    ")\n",
    "\n",
    "stack.fit(X_train, y_train)\n",
    "stack_preds = stack.predict(X_test)\n",
    "print(f\"Stacked RMSE: {np.sqrt(mean_squared_error(y_test, stack_preds)):.4f}\")\n",
    "print(f\"Stacked R²: {r2_score(y_test, stack_preds):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
