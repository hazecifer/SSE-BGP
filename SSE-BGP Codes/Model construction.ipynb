{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patheffects as path_effects\n",
    "import math\n",
    "import random\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_1 = 'Encoded_train_data.csv'\n",
    "X_train_encoded = pd.read_csv(file_path_1)\n",
    "file_path_2 = 'Encoded_test_data.csv'\n",
    "X_test_encoded = pd.read_csv(file_path_2)\n",
    "file_path_3 = 'y_train.csv'\n",
    "y_train = pd.read_csv(file_path_3)\n",
    "file_path_4 = 'y_test.csv'\n",
    "y_test = pd.read_csv(file_path_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learners = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=300, max_depth=13, min_samples_leaf=1, min_samples_split=2, random_state=42)),\n",
    "    ('xgb', XGBRegressor(n_estimators=257, learning_rate=0.125, max_depth=3, colsample_bytree=1, subsample=0.5, random_state=42)),\n",
    "    ('catboost', CatBoostRegressor(\n",
    "        iterations=300,\n",
    "        learning_rate=0.191,\n",
    "        depth=3,\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    )),\n",
    "    ('gbr', GradientBoostingRegressor(n_estimators=277, learning_rate=0.069, max_depth=4, subsample=0.5, random_state=42)),\n",
    "    ('lgbm', LGBMRegressor(n_estimators=300, learning_rate=0.165, max_depth=3, num_leaves=20, random_state=42)),\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, input_dim, hidden_dim=64, dropout=0.2, lr=0.001, epochs=100, batch_size=32):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.build_model().to(self.device)\n",
    "        self.history = {'loss': []}\n",
    "        \n",
    "    def build_model(self):\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dim//2, 1)\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model.train()\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item() * batch_X.size(0)\n",
    "            epoch_loss /= len(loader.dataset)\n",
    "            self.history['loss'].append(epoch_loss)\n",
    "            if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "                print(f'Epoch [{epoch+1}/{self.epochs}], Loss: {epoch_loss:.4f}')\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "            outputs = self.model(X_tensor)\n",
    "            return outputs.cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_learner = PyTorchRegressor(\n",
    "    input_dim=len(base_learners),\n",
    "    hidden_dim=256,\n",
    "    dropout=0.05,\n",
    "    lr=0.00023,\n",
    "    epochs=200,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "stacking_regressor = StackingRegressor(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=20,\n",
    "    n_jobs=-1,\n",
    "    passthrough=False  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_regressor.fit(X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_stack = stacking_regressor.predict(X_test_encoded)\n",
    "\n",
    "mse_stack = mean_squared_error(y_test, y_pred_stack)\n",
    "r2_stack = r2_score(y_test, y_pred_stack)\n",
    "mae_stack = mean_absolute_error(y_test,y_pred_stack)\n",
    "rmse_stack= np.sqrt(mean_squared_error(y_test, y_pred_stack))\n",
    "\n",
    "print(f\"MSE:{mse_stack:.4f}\")\n",
    "print(f\"MAE:{mae_stack:.4f}\")\n",
    "print(f\"RMSE:{rmse_stack:.4f}\")\n",
    "print(f\"RÂ²;{r2_stack:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = stacking_regressor.predict(X_train_encoded)\n",
    "y_pred_test = stacking_regressor.predict(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred_stack\n",
    "\n",
    "counts, bins = np.histogram(residuals, bins=30, density=True)\n",
    "bin_centers = 0.5 * (bins[1:] + bins[:-1])\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin=min(counts), vmax=max(counts))\n",
    "cmap = plt.get_cmap('Purples')\n",
    "colors = cmap(norm(counts))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bars = ax.bar(bin_centers, counts, width=(bins[1]-bins[0]), color=colors, edgecolor='black', alpha=0.7, label='Residuals')\n",
    "\n",
    "sns.kdeplot(residuals, color='darkblue', linewidth=2, ax=ax, label='KDE')\n",
    "\n",
    "sm = mpl.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])  \n",
    "cbar = fig.colorbar(sm, ax=ax)\n",
    "cbar.set_label('Density', fontsize=12)\n",
    "ax.set_title('Residuals Distribution', fontsize=18, fontweight='bold')\n",
    "ax.set_xlabel('Residuals', fontsize=14)\n",
    "ax.set_ylabel('Density', fontsize=14)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.2f}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),  # 3 points vertical offset\n",
    "                textcoords='offset points',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "mean_residual = np.mean(residuals)\n",
    "std_residual = np.std(residuals)\n",
    "ax.axvline(mean_residual, color='red', linestyle='--', linewidth=1)\n",
    "ax.text(mean_residual, max(counts)*0.9, f'Mean: {mean_residual:.4f}', color='red', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# plt.savefig('residuals_distribution.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
